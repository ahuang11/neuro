{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Large - Multi-Channel Timeseries with Dynamic Data Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO create banner image\n",
    "![]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition alert alert-info\">\n",
    "    <p class=\"admonition-title\" style=\"font-weight:bold\"> Visit the Index Page </p>\n",
    "    This workflow example is part of set of related workflows. If you haven't already, visit the <a href=\"/index.html\">index</a> page for an introduction and guidance on choosing the appropriate workflow.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intended use-case for this workflow is to browse and annotate multi-channel timeseries data from an [electrophysiological](https://en.wikipedia.org/wiki/Electrophysiology) recording session.\n",
    "\n",
    "Compared to other approaches in this set of workflows, this particular workflow is focused on 'large-sized' datasets, which we define as a dataset that does not comfortably fit into the available RAM.\n",
    "\n",
    "In such cases where the entire dataset cannot be loaded into memory, we have to consider what approaches might work best for scalability. The approach we will demonstrate is one of the most common approaches in the bio-imaging community, and is based on the use of multi-resolution data structures.\n",
    "\n",
    "We will create a derived dataset that includes a multi-resolution pyramid (incrementally downsampled versions of a large dataset), and then use a dynamic accessor to access the appropriate resolution based on viewport and screen parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Resources\n",
    "\n",
    "| Topic | Type | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Intro and Guidance](./index.ipynb) | Prerequisite | Background |\n",
    "| [Time Range Annotation](./time_range_annotation.ipynb) | Next Step | Display and edit time ranges |\n",
    "| [Smaller Dataset Workflow](./small_multi-chan-ts.ipynb) | Alternative | Use Numpy |\n",
    "| [Medium Dataset Workflow](./medium_multi-chan-ts.ipynb) | Alternative | Use Pandas and downsampling |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "import datatree as dt\n",
    "from ndpyramid import pyramid_create\n",
    "from tsdownsample import MinMaxLTTBDownsampler\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import panel as pn\n",
    "import datatree as dt\n",
    "import holoviews as hv\n",
    "from scipy.stats import zscore\n",
    "from holoviews.plotting.links import RangeToolLink\n",
    "from holoviews.operation.datashader import rasterize\n",
    "from bokeh.models.tools import WheelZoomTool, HoverTool\n",
    "\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _help_downsample(data, time, n_out):\n",
    "    indices = MinMaxLTTBDownsampler().downsample(time, data, n_out=n_out)\n",
    "    return data[indices], indices\n",
    "\n",
    "\n",
    "def apply_downsample(ts_ds, factor, dims):\n",
    "    dim = dims[0]\n",
    "    n_out = len(ts_ds[\"data\"]) // factor\n",
    "    ts_ds_downsampled, indices = xr.apply_ufunc(\n",
    "        _help_downsample,\n",
    "        ts_ds[\"data\"],\n",
    "        ts_ds[dim],\n",
    "        kwargs=dict(n_out=n_out),\n",
    "        input_core_dims=[[dim], [dim]],\n",
    "        output_core_dims=[[dim], [\"indices\"]],\n",
    "        exclude_dims=set((dim,)),\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "        dask_gufunc_kwargs=dict(output_sizes={dim: n_out, \"indices\": n_out}),\n",
    "    )\n",
    "    ts_ds_downsampled[dim] = ts_ds[dim].isel(time=indices.values[0])\n",
    "    return ts_ds_downsampled.rename(\"data\")\n",
    "\n",
    "\n",
    "def build_dataset(f, data_key, dims):\n",
    "    coords = {f[dim] for dim in dims.values()}\n",
    "    data = f[data_key]\n",
    "    ds = xr.DataArray(\n",
    "        da.from_array(data, name=\"data\", chunks=(data.shape[0], 1)),\n",
    "        dims=dims,\n",
    "        coords=coords,\n",
    "    ).to_dataset()\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir='~/data/allen/'\n",
    "data_dir = os.path.expanduser(data_dir)\n",
    "\n",
    "f = h5py.File(f\"{data_dir}/probe_810755797_lfp.nwb\", \"r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_ds = build_dataset(\n",
    "    f,\n",
    "    \"acquisition/probe_810755797_lfp_data/data\",\n",
    "    {\n",
    "        \"time\": \"acquisition/probe_810755797_lfp_data/timestamps\",\n",
    "        \"channel\": \"acquisition/probe_810755797_lfp_data/electrodes\",\n",
    "    },\n",
    ").isel(channel=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ts_dt = pyramid_create(\n",
    "    ts_ds,\n",
    "    factors=[1, 2, 4, 8, 16, 32, 64, 128],\n",
    "    dims=[\"time\"],\n",
    "    func=apply_downsample,\n",
    "    type_label=\"pick\",\n",
    "    method_label=\"pyramid_downsample\",\n",
    ")\n",
    "ts_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYRAMID_FILE = 'pyramid_neuropix_10s.zarr'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ts_dt.to_zarr(PYRAMID_FILE, mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ts_dt = dt.open_datatree(PYRAMID_FILE, engine=\"zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHANNELS = 20\n",
    "X_PADDING = 0.2  # buffer-padding % for the x_range time slice loading\n",
    "\n",
    "def _extract_ds(ts_dt, level, channel):\n",
    "    ds = (\n",
    "        ts_dt[str(level)]\n",
    "        .sel(channel=channel)\n",
    "        .ds.swap_dims({\"time\": \"multi_time\"})\n",
    "        .rename({\"multi_time\": \"time\"})\n",
    "        .set_xindex('time') # needed for sel(time=..)\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def rescale(x_range, y_range, width, scale, height):\n",
    "    import time\n",
    "\n",
    "    s = time.perf_counter()\n",
    "    print(f\"- Update triggered! {width=} {x_range=}\")\n",
    "    if x_range is None:\n",
    "        x_range = time_da.min().item(), time_da.max().item()\n",
    "    if y_range is None:\n",
    "        y_range = 0, num_channels\n",
    "    x_padding = (x_range[1] - x_range[0]) * X_PADDING\n",
    "    time_slice = slice(x_range[0] - x_padding, x_range[1] + x_padding)\n",
    "\n",
    "    if width is None or height is None:\n",
    "        zoom_level = num_levels - 1\n",
    "        size = data.size\n",
    "    else:\n",
    "        print('here')\n",
    "        sizes = [\n",
    "            _extract_ds(ts_dt, zoom_level, 0)[\"time\"].sel(time=time_slice).size\n",
    "            for zoom_level in range(num_levels)\n",
    "        ]\n",
    "        zoom_level = np.argmin(np.abs(np.array(sizes) - width))\n",
    "        size = sizes[zoom_level]\n",
    "    e = time.perf_counter()\n",
    "    print(f\"Zoom level computation took {e-s:.2f}s\")\n",
    "\n",
    "    title = (\n",
    "        f\"level {zoom_level} ({x_range[0]:.2f}s - {x_range[1]:.2f}s) \"\n",
    "        f\"(WxH: {width}x{height}) (length: {size})\"\n",
    "    )\n",
    "\n",
    "    if zoom_level == pn.state.cache.get(\"current_zoom_level\") and pn.state.cache.get(\n",
    "        \"curves\"\n",
    "    ):\n",
    "        cached_x_range = pn.state.cache[\"x_range\"]\n",
    "        if x_range[0] >= cached_x_range[0] and x_range[1] <= cached_x_range[1]:\n",
    "            print(f\"Using cached curves! {zoom_level=}\")\n",
    "            if x_range != cached_x_range:\n",
    "                print(f\"Different x_range: {x_range} {cached_x_range}\")\n",
    "            return pn.state.cache[\"curves\"].opts(title=title)\n",
    "\n",
    "    curves = hv.Overlay(kdims=\"Channel\")\n",
    "    for channel in channels:\n",
    "        hover = HoverTool(\n",
    "            tooltips=[\n",
    "                (\"Channel\", str(channel)),\n",
    "                (\"Time\", \"$x s\"),\n",
    "                (\"Amplitude\", \"$y ÂµV\"),\n",
    "            ]\n",
    "        )\n",
    "        sub_ds = _extract_ds(ts_dt, zoom_level, channel).sel(time=time_slice).load()\n",
    "        curve = hv.Curve(sub_ds, [\"time\"], [\"data\"], label=f\"ch{channel}\").opts(\n",
    "            color=\"black\",\n",
    "            line_width=1,\n",
    "            subcoordinate_y=True,\n",
    "            subcoordinate_scale=1,\n",
    "            default_tools=[\"pan\", \"reset\", WheelZoomTool(), hover],\n",
    "        )\n",
    "        curves *= curve\n",
    "    print(f\"Overlaying curves took {time.perf_counter()-e:.2f}s\")\n",
    "\n",
    "    curves = curves.opts(\n",
    "        xlabel=\"Time (s)\",\n",
    "        ylabel=\"Channel\",\n",
    "        title=title,\n",
    "        show_legend=False,\n",
    "        padding=0,\n",
    "        aspect=1.5,\n",
    "        responsive=True,\n",
    "        framewise=True,\n",
    "        axiswise=True,\n",
    "    )\n",
    "    pn.state.cache[\"current_zoom_level\"] = zoom_level\n",
    "    pn.state.cache[\"x_range\"] = x_range\n",
    "    pn.state.cache[\"curves\"] = curves\n",
    "    print(f\"Using updated curves! {x_range} {zoom_level}\\n\\n\")\n",
    "    return curves\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_stream = hv.streams.RangeXY()\n",
    "size_stream = hv.streams.PlotSize()\n",
    "dmap = hv.DynamicMap(rescale, streams=[size_stream, range_stream])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ts_dt = dt.open_datatree(PYRAMID_FILE, engine=\"zarr\").sel(\n",
    "    channel=slice(0, MAX_CHANNELS)\n",
    ")\n",
    "\n",
    "\n",
    "# TODO we need to avoid loading in the largest (\"0\") data array:\n",
    "# `data = ts_dt[\"0\"].ds[\"data\"].values.T` if it's going to significantly slow things down\n",
    "# or even worse. However, we want don't want an overly-decimated array as a minimap image.\n",
    "I'm not sure how to handle this yet.\n",
    "sel_group = '2' #ts_dt.groups[-1]\n",
    "num_levels = len(ts_dt)\n",
    "time_da = _extract_ds(ts_dt, sel_group, 0)[\"time\"]\n",
    "channels = ts_dt[sel_group].ds[\"channel\"].values\n",
    "num_channels = len(channels)\n",
    "data = ts_dt[sel_group].ds[\"data\"].values.T\n",
    "\n",
    "y_positions = range(num_channels)\n",
    "yticks = [(i, ich) for i, ich in enumerate(channels)]\n",
    "z_data = zscore(data.T, axis=1)\n",
    "\n",
    "minimap = rasterize(\n",
    "    hv.Image((time_da, y_positions, z_data), [\"Time (s)\", \"Channel\"], \"Amplitude (uV)\")\n",
    ")\n",
    "\n",
    "minimap = minimap.opts(\n",
    "    cnorm='eq_hist',\n",
    "    cmap=\"RdBu_r\",\n",
    "    xlabel=\"\",\n",
    "    yticks=[yticks[0], yticks[-1]],\n",
    "    toolbar=\"disable\",\n",
    "    height=120,\n",
    "    responsive=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tool_link = RangeToolLink(\n",
    "    minimap,\n",
    "    dmap,\n",
    "    axes=[\"x\", \"y\"],\n",
    "    boundsx=(0, time_da.max().item() // 2),\n",
    "    boundsy=(0, len(channels) // 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "app = (dmap + minimap).cols(1)\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Optional:* Standalone App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using HoloViz Panel, we can also set this application as servable so we can see it in a browser window, outside of a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.template.FastListTemplate(main=[app]).servable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## full app for easy copy/pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provided as a markdown block so it doesn't execute when running this notebook\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro-multi-chan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
