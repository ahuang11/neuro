{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Viewer with synthetic data pipeline\n",
    "![status](https://img.shields.io/badge/status-in%20progress-orange)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./assets/230524_eeg-viewer.png\" alt=\"eeg viewer preview\" width=\"450\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This workflow is intended to demonstrate the visualization of a set of 1D EEG timeseries with HoloViz and Bokeh tools.\n",
    "\n",
    "For details specific to this workflow, such as goals, specifications, and bottlenecks, please see this workflow's [readme](./readme_eeg-viewer.md).\n",
    "\n",
    "For a summary of EEG research, data, and software, see [neuro/wiki/EEG-notes](https://github.com/holoviz-topics/neuro/wiki/EEG-notes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition alert alert-info\">\n",
    "    <p class=\"admonition-title\" style=\"font-weight:bold\">Requirements</p>\n",
    "    <p>This workflow notebook requires the <a href=\"./environment.yml\">environment</a> specified in this workflow directory.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np; np.random.seed(0)\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Viz\n",
    "# import colorcet as cc\n",
    "import holoviews as hv; hv.extension('bokeh')\n",
    "from holoviews.plotting.links import RangeToolLink\n",
    "# from holoviews.operation.datashader import rasterize\n",
    "from holoviews import Dataset\n",
    "from bokeh.models import HoverTool, WheelZoomTool\n",
    "import panel as pn; pn.extension(template='material')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition alert alert-warning\">\n",
    "    <p class=\"admonition-title\" style=\"font-weight:bold\">Run a single option</p>\n",
    "    <p>... from the following data generation options</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 25\n",
    "n_seconds = 15\n",
    "fs = 250  # Sampling frequency\n",
    "\n",
    "generate_random = False\n",
    "generate_sin = False\n",
    "generate_realistic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Random cumsum data. Useful for bug reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_random:\n",
    "    import numpy as np; np.random.seed(0)\n",
    "    \n",
    "    # n_channels = 25\n",
    "    # n_seconds = 30\n",
    "    # fs = 512\n",
    "    \n",
    "    total_samples = fs*n_seconds\n",
    "    time = np.linspace(0, n_seconds, total_samples)\n",
    "    data = np.random.randn(n_channels, total_samples).cumsum(axis=1)\n",
    "    channels = [f\"EEG {i}\" for i in range(n_channels)]\n",
    "    print(f'shape: {data.shape} (n_channels, samples) ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Increasing sin data. Useful for quick demo and debugging minimap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_sin: \n",
    "    # n_channels = 25\n",
    "    # n_seconds = 30\n",
    "    # fs = 512  # Sampling frequency\n",
    "    \n",
    "    init_freq = 1  # Initial frequency in Hz\n",
    "    freq_inc = 20/n_channels  # Frequency increment\n",
    "    amplitude = 1\n",
    "    \n",
    "    total_samples = n_seconds * fs\n",
    "    time = np.linspace(0, n_seconds, total_samples)\n",
    "    channels = [f'EEG {i}' for i in range(n_channels)]\n",
    "    \n",
    "    data = np.array([amplitude * np.sin(2 * np.pi * (init_freq + i * freq_inc) * time)\n",
    "                     for i in range(n_channels)])\n",
    "    print(f'shape: {data.shape} (n_channels, samples) ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semi-realistic data. Useful for demo\n",
    "\n",
    "The `generate_eeg_powerlaw` function synthesizes EEG data as high-pass filtered pink noise power law time series by default. The function returns a 2D numpy array of synthetic EEG data (in microvolts) shaped as (number of channels, total samples), a 1D time array (in seconds), and a list of channel names. Parameters such as the high-pass filter factor (in Hz) and an amplitude scaling factor allow customization of the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_realistic:\n",
    "    from neurodatagen.eeg import generate_eeg_powerlaw\n",
    "    # from hvneuro import download_file\n",
    "    \n",
    "    data, time, channels = generate_eeg_powerlaw(n_channels, n_seconds, fs, channel_prefix='EEG', blink_scale=.009)\n",
    "    print(f'shape: {data.shape} (n_channels, samples) ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate range annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import colorcet as cc\n",
    "import string\n",
    "\n",
    "def create_range_annotations(n_total_seconds: int, n_categories: int, \n",
    "                             n_total_annotations: int, duration: int = 1) -> pd.DataFrame:\n",
    "\n",
    "    start_times = np.sort(np.random.randint(0, n_total_seconds - duration, n_total_annotations))\n",
    "    \n",
    "    # Ensure the annotations are non-overlapping\n",
    "    for i in range(1, len(start_times)):\n",
    "        if start_times[i] < start_times[i-1] + duration:\n",
    "            start_times[i] = start_times[i-1] + duration\n",
    "    end_times = start_times + duration\n",
    "    categories = np.random.choice(list(string.ascii_uppercase)[:n_categories], n_total_annotations)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'start': start_times,\n",
    "        'end': end_times,\n",
    "        'category': categories\n",
    "    })\n",
    "    df['category'] = df['category'].astype('category')\n",
    "    \n",
    "    unique_categories = df['category'].cat.categories\n",
    "    color_map = dict(zip(unique_categories, cc.glasbey[:len(unique_categories)]))\n",
    "    df['color'] = df['category'].map(color_map)\n",
    "    df['color'] = df['color'].astype('category')\n",
    "    \n",
    "    return df\n",
    "\n",
    "n_categories = 3\n",
    "n_total_annotations = 5\n",
    "annotations_df = create_range_annotations(n_seconds, n_categories, n_total_annotations)\n",
    "annotations_df.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize synthetic data. Approach: Subcoords, HoloViz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach makes use HoloViews API that adds axis offset automation and a (hopefully) simpler API to Bokeh's experimental 'subplot' functionality, as described and demonstrated separately.\n",
    "\n",
    "Pros:\n",
    "- Allows for independent dynamic y-axis scaling of individual traces\n",
    "- Simpler on the front end to deal with offsets; just set `subcoordinate_y = True`.\n",
    "\n",
    "Cons:\n",
    "- Multiple levels of being experimental, so still has some bugs to work out\n",
    "- Requires HoloViews >=1.18 and 1.18 is not yet supporting Bokeh 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition alert alert-info\">\n",
    "    <p class=\"admonition-title\" style=\"font-weight:bold\">Requires HoloViews>=1.18</p>\n",
    "    <p>... for initial `subcoordinate_y` support. Later versions will include important improvements.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xzoom_out_extent = 2\n",
    "start_t_disp = 4.5 #time[0] # start time of initially displayed window \n",
    "max_t_disp = xzoom_out_extent # max time in seconds to initially display\n",
    "max_ch_disp = 20  # max channels to initially display\n",
    "max_y_disp = np.min((max_ch_disp - 1.5, n_channels - 1.5))\n",
    "subcoord_btm = -0.5 # auto lower xlim of first subcoord\n",
    "clim_mul = 1 # color limit multiplier.. adjusts the levels on the minimap\n",
    "\n",
    "annotation_elements = [hv.VSpan(row['start'], row['end']).opts(fill_color=row['color'], alpha=0.2, line_alpha=0) \n",
    "                       for _, row in annotations_df.iterrows()]\n",
    "annotations_overlay = hv.Overlay(annotation_elements)\n",
    "\n",
    "hover = HoverTool(tooltips=[\n",
    "    (\"Channel\", \"@channel\"),\n",
    "    (\"Time\", \"$x s\"),\n",
    "    (\"Amplitude\", \"$y ÂµV\")])\n",
    "\n",
    "channel_curves = []\n",
    "for channel, channel_data in zip(channels, data):\n",
    "    ds = Dataset((time, channel_data, channel), [\"Time\", \"Amplitude\", \"channel\"])\n",
    "    curve = hv.Curve(ds, \"Time\", [\"Amplitude\", \"channel\"], label=f'{channel}')\n",
    "    curve.opts(color=\"black\", line_width=1, subcoordinate_y=True, tools=[hover])\n",
    "    channel_curves.append(curve)\n",
    "\n",
    "eeg_viewer = (hv.Overlay(channel_curves, kdims=\"Channel\") * annotations_overlay)\n",
    "eeg_viewer = eeg_viewer.opts(\n",
    "    xlabel=\"Time (s)\", ylabel=\"Channel\", show_legend=False,\n",
    "    padding=0, aspect=1.5, responsive=True, shared_axes=False,\n",
    "     #ylim does not work with subcoordinate_y\n",
    "    # xlim=(start_t_disp, start_t_disp+max_t_disp), ylim=(subcoord_btm, subcoord_btm+max_y_disp),\n",
    "    backend_opts={\n",
    "        \"y_range.start\": subcoord_btm, # required as long as ylim doesn't work\n",
    "        \"y_range.end\": subcoord_btm + max_y_disp, # required as long as ylim doesn't work\n",
    "        \"x_range.start\": start_t_disp,\n",
    "        \"x_range.end\": start_t_disp + max_t_disp,\n",
    "        \"x_range.bounds\": (time.min(), time.max()), # absolute outer limits on pan/zoom\n",
    "        \"y_range.bounds\": (0, len(channels)),\n",
    "        \"x_range.max_interval\": xzoom_out_extent\n",
    "    })\n",
    "\n",
    "y_positions = range(len(channels))\n",
    "yticks = [(i, ich) for i, ich in enumerate(channels)]\n",
    "z_data = zscore(data, axis=1) # np.zeros_like(data)\n",
    "# Does not currently work with rasterize on the minimap image\n",
    "minimap = hv.Image((time, y_positions, z_data), [\"Time (s)\", \"Channel\"], \"Amplitude (uV)\")\n",
    "minimap = minimap.opts(\n",
    "    cmap=\"RdBu_r\", colorbar=False, xlabel='',\n",
    "    alpha=.3, yticks=[yticks[0], yticks[-1]],\n",
    "    toolbar='disable', # needed to prevent zoom and pan behavior on image\n",
    "    height=120, responsive=True, default_tools=[],\n",
    "    clim=(-z_data.std()*clim_mul, z_data.std()*clim_mul))\n",
    "\n",
    "RangeToolLink(minimap, eeg_viewer, axes=[\"x\", \"y\"],\n",
    "              boundsx=(start_t_disp, start_t_disp + max_t_disp), #required for reset behavior\n",
    "              boundsy=(subcoord_btm, subcoord_btm + max_y_disp) #required for reset behavior\n",
    "             )\n",
    "\n",
    "eeg_app = (eeg_viewer + minimap * annotations_overlay).opts().cols(1)\n",
    "eeg_app = pn.Column((eeg_viewer + minimap * annotations_overlay).cols(1), min_height=650)\n",
    "eeg_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize synthetic data. Approach: Offset, HoloViz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach makes a copy of the data with an **offset** to position the timeseries traces vertically stacked. The original copy of the data is used to provide accurate y-axis hover information over each trace.\n",
    "\n",
    "Pros:\n",
    "- Simpler on the backend, as your just plotting multiple timeseries on the same coordinate axis\n",
    "- More amenable to existing datashader rendering\n",
    "- Uses high-level HoloViz\n",
    "\n",
    "Cons:\n",
    "- More complicated on the front-end, as you have to manually create and control the offset data\n",
    "- Makes a copy of the data\n",
    "- Does not allow for independent dynamic y-axis scaling of individual traces without rerendering the whole canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    max_ch_disp = 15  # max channels to initially display\n",
    "    max_t_disp = 10 # max time in seconds to initially display\n",
    "    spacing = 5.5  # Spacing between channels\n",
    "    clim_mul = 3.2 # color range multiplier for minimap. lower will saturate more.\n",
    "    \n",
    "    annotation_elements = [hv.VSpan(row['start'], row['end']).opts(fill_color=row['color'], alpha=0.2, line_alpha=0) \n",
    "                           for _, row in annotations_df.iterrows()]\n",
    "    annotations_overlay = hv.Overlay(annotation_elements)\n",
    "    \n",
    "    # Create a hv.Curve element per chan\n",
    "    channel_curves = []\n",
    "    max_data = data.max()\n",
    "     \n",
    "    hover = HoverTool(tooltips=[\n",
    "        (\"Channel\", \"@channel\"),\n",
    "        (\"Time\", \"$x s\"),\n",
    "        (\"Amplitude\", \"@original_amplitude ÂµV\")])\n",
    "    \n",
    "    offset = np.std(data) * spacing\n",
    "    for i, channel_data in enumerate(data):\n",
    "        offset_data = channel_data + (i * offset)\n",
    "        max_data = max(offset_data.max(), max_data) # update max\n",
    "        ds = Dataset((time, offset_data, channel_data, channels[i]), [\"Time\", \"Amplitude\", \"original_amplitude\", \"channel\"])\n",
    "        channel_curves.append(\n",
    "            hv.Curve(ds, \"Time\", [\"Amplitude\", \"original_amplitude\", \"channel\"]).opts(\n",
    "                color=\"black\", line_width=1,\n",
    "                tools=[hover, 'xwheel_zoom'], shared_axes=False))\n",
    "    \n",
    "    yticks = [(i * offset, ich) for i, ich in enumerate(channels)]\n",
    "    \n",
    "    # set maintain focus to False to allow independence for zoom out against a single hardbound\n",
    "    # def set_maintain_focus(plot, element):\n",
    "    #     wheel_zoom = plot.state.select(type=WheelZoomTool)\n",
    "    #     if wheel_zoom:\n",
    "    #         wheel_zoom[0].maintain_focus = False\n",
    "            \n",
    "    # Create an overlay of curves\n",
    "    eeg_viewer = (annotations_overlay * hv.Overlay(channel_curves, kdims=\"Channel\")).opts(\n",
    "        padding=0, xlabel=\"Time (s)\", ylabel=\"Channel\", #default_tools=['hover', 'pan', 'box_zoom', 'save', 'reset'],\n",
    "        yticks=yticks, show_legend=False, aspect=1.5, responsive=True,\n",
    "        shared_axes=False, backend_opts={\n",
    "            \"x_range.bounds\": (time.min(), time.max()),\n",
    "            \"y_range.bounds\": (data.min(), max_data)})\n",
    "    \n",
    "    # Minimap\n",
    "    y_positions, _ = zip(*yticks) # use positions of yticks for yaxis of minimap image\n",
    "    z_data = zscore(data, axis=1)\n",
    "    \n",
    "    # fix rasterize\n",
    "    minimap = hv.Image((time, y_positions, z_data), [\"Time (s)\", \"Channel\"], \"Amplitude (uV)\")\n",
    "    minimap = minimap.opts(\n",
    "        cmap=\"RdBu_r\", colorbar=False, xlabel='', alpha=.8, yticks=[yticks[0], yticks[-1]],\n",
    "        height=100, responsive=True, default_tools=[''], shared_axes=False, clim=(-z_data.std()*clim_mul, z_data.std()*clim_mul))\n",
    "        \n",
    "    # Create RangeToolLink between the minimap and the main EEG viewer \n",
    "    # (quirk: apply to just one eeg trace and it will apply to all. see HoloViews #4472)\n",
    "    max_y_disp = np.max(data[max_ch_disp-1,:] + ((max_ch_disp-1) * offset))\n",
    "    RangeToolLink(minimap, list(eeg_viewer.values())[0], axes=[\"x\", \"y\"],\n",
    "                  boundsx=(None, max_t_disp),\n",
    "                  boundsy=(None, max_y_disp))\n",
    "    \n",
    "    # eeg_app = pn.Column((eeg_viewer + minimap * annotation).cols(1), min_height=650).servable(target='main', title='EEG Viewer with HoloViz and Bokeh')\n",
    "    eeg_app = (eeg_viewer + minimap * annotations_overlay).cols(1)\n",
    "    eeg_app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize synthetic data. Approach: Subcoords, Bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach makes use of Bokeh's experimental 'subplot' functionality, which allows for nesting subcoordinate systems so that each timeseries is on its own y-axis.\n",
    "\n",
    "Pros:\n",
    "- Allows for independent dynamic y-axis scaling of individual traces\n",
    "- Theoretically simpler on the front end (however, this is only really true if the offset of the subcoordinate axes are automatically handled, which is not yet true the Subcoords-Bokeh approach, but it is true of the Subcoords-HoloViews approach)\n",
    "\n",
    "Cons:\n",
    "- More complicated on the front and back-end, as you have to handle the renderers and zoom tool level manually\n",
    "- Pure, lower-level Bokeh (maybe this is a pro for some)\n",
    "- Requires Bokeh >=3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition alert alert-info\">\n",
    "    <p class=\"admonition-title\" style=\"font-weight:bold\">Requires Bokeh>=3.3</p>\n",
    "    <p>...primarily for subplot zooming, but also fixes to minimap box annotation handling</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    from scipy.stats import zscore\n",
    "    \n",
    "    from bokeh.core.properties import field\n",
    "    from bokeh.io import show, output_notebook\n",
    "    from bokeh.layouts import column, row\n",
    "    from bokeh.models import (ColumnDataSource, CustomJS, Div, FactorRange, HoverTool,\n",
    "                              Range1d, Switch, WheelZoomTool, ZoomInTool, ZoomOutTool, \n",
    "                              RangeTool)\n",
    "    from bokeh.palettes import Category10\n",
    "    from bokeh.plotting import figure\n",
    "    from bokeh.models import FixedTicker\n",
    "    \n",
    "    output_notebook()\n",
    "    \n",
    "    # n_channels = 10\n",
    "    # n_seconds = 100\n",
    "    max_ch_disp = n_channels/2  # max channels to initially display\n",
    "    max_t_disp = n_seconds/2 # max time in seconds to initially display\n",
    "    # fs = 512 # Hz\n",
    "    # total_samples = fs*n_seconds\n",
    "    time = np.linspace(0, n_seconds, total_samples)\n",
    "    data = np.random.randn(n_channels, total_samples).cumsum(axis=1)\n",
    "    # channels = [f\"EEG {i}\" for i in range(n_channels)]\n",
    "    \n",
    "    hover = HoverTool(tooltips=[\n",
    "        (\"Channel\", \"$name\"),\n",
    "        (\"Time\", \"$x s\"),\n",
    "        (\"Amplitude\", \"$y ÂµV\"),\n",
    "    ])\n",
    "    \n",
    "    x_range = Range1d(start=time.min(), end=time.max())\n",
    "    y_range = Range1d(start=-0.5, end=len(channels) - 1 + 0.5)\n",
    "    p = figure(x_range=x_range, y_range=y_range, height=500, width=800,\n",
    "               x_axis_label='Time (s)',\n",
    "               lod_threshold=None, tools=\"pan, reset\")\n",
    "    \n",
    "    source = ColumnDataSource(data=dict(time=time))\n",
    "    renderers = []\n",
    "    \n",
    "    for i, channel in enumerate(channels):\n",
    "        \n",
    "        xy = p.subplot(\n",
    "            x_source=p.x_range,\n",
    "            y_source=Range1d(start=data[i].min(), end=data[i].max()),\n",
    "            x_target=p.x_range,\n",
    "            y_target=Range1d(start=i - 0.5, end=i + 0.5),\n",
    "        )\n",
    "    \n",
    "        source.data[channel] = data[i]\n",
    "        line = xy.line(field(\"time\"), field(channel), color='black', source=source, name=channel)\n",
    "        renderers.append(line)\n",
    "    \n",
    "    ticks = list(range(len(channels)))\n",
    "    p.yaxis.ticker = FixedTicker(ticks=ticks)\n",
    "    p.yaxis.major_label_overrides = {i: f\"EEG {i}\" for i in ticks}\n",
    "    \n",
    "    level = 1\n",
    "    \n",
    "    ywheel_zoom = WheelZoomTool(renderers=renderers, level=level, dimensions=\"height\")\n",
    "    xwheel_zoom = WheelZoomTool(renderers=renderers, level=level, dimensions=\"width\")\n",
    "    yzoom_in = ZoomInTool(renderers=renderers, level=level, dimensions=\"height\")\n",
    "    yzoom_out = ZoomOutTool(renderers=renderers, level=level, dimensions=\"height\")\n",
    "    \n",
    "    p.add_tools(ywheel_zoom, xwheel_zoom, yzoom_in, yzoom_out, hover)\n",
    "    p.toolbar.active_scroll = ywheel_zoom\n",
    "    \n",
    "    z_data = zscore(data, axis=1)\n",
    "    \n",
    "    range_tool = RangeTool(x_range=p.x_range, y_range=p.y_range)\n",
    "    range_tool.x_range.update(start=0, end=max_t_disp)\n",
    "    range_tool.y_range.update(start=0, end=max_ch_disp)\n",
    "    range_tool.overlay.fill_alpha = .8\n",
    "    \n",
    "    select = figure(height=120, width=800, tools=\"\", toolbar_location=None, y_axis_type=None,\n",
    "                    x_range=(time.min(), time.max()),\n",
    "                    y_range=(-0.5, len(channels) - 1 + 0.5))\n",
    "    select.image(image=[z_data], x=0, y=0, dw=n_seconds, dh=n_channels, palette=\"Sunset11\")\n",
    "    select.add_tools(range_tool)\n",
    "    \n",
    "    show(column(p, select))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
