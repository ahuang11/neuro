{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b7597-eb68-4633-b260-7fa0ef77bf8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea3e8487-91ad-4608-bb47-3fdc1fc74a2f",
   "metadata": {},
   "source": [
    "A more up to date version is in 230818_allen_LFP_NWB-Zarr_plotBokeh.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5268034-59d2-44e3-bf52-ae82bbea641c",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- [ ] add switch for overwriting zarr dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168fcac-f8a3-4f96-be67-3d9eda49750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_filepath = os.path.join(data_dir, f\"probe_{probe_id}_lfp.nwb\")\n",
    "\n",
    "# with h5py.File(input_filepath, \"r\") as f:\n",
    "#     electrodes_table = f[\"general/extracellular_ephys/electrodes\"]\n",
    "#     electrode_metadata = {col_name: electrodes_table[col_name][:] for col_name in electrodes_table.keys()}\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# channel_multiindex = pd.MultiIndex.from_frame(pd.DataFrame(electrode_metadata), names=electrode_metadata.keys())\n",
    "\n",
    "# pd.DataFrame(electrode_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56126f3-f56b-4416-a429-be80a60d4776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_zarr_file_approach3(probe_id: str, data_dir: str, compress_data: bool = False, chunk_size: tuple = (1_000_000, 20), force_rechunk: bool = True):\n",
    "#     \"\"\"\n",
    "#     Create a Zarr directory containing a single probe's data, chunked and optionally compressed.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     probe_id : str\n",
    "#         The probe ID used to identify the specific probe.\n",
    "#     data_dir : str\n",
    "#         The base data directory where the input and output files are located.\n",
    "#     compress_data : bool, optional\n",
    "#         Whether to compress the LFP data. Compression reduces file size at the cost of additional computation.\n",
    "#         Defaults to False.\n",
    "#     chunk_size : tuple, optional\n",
    "#         The size of chunks for the LFP data. Defaults to (1_000_000, 20) : (time_samples, channels).\n",
    "#     force_rechunk : bool, optional\n",
    "#         Whether to force rechunking with the provided chunk size, even if the HDF5 dataset is already chunked.\n",
    "#         Defaults to True.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     ds : xr.Dataset\n",
    "#         An Xarray Dataset object containing the LFP data and associated coordinates. The actual data is not loaded into\n",
    "#         memory until explicitly accessed.\n",
    "#     \"\"\"\n",
    "\n",
    "#     input_filepath = os.path.join(data_dir, f\"probe_{probe_id}_lfp.nwb\")\n",
    "#     output_filepath = os.path.join(data_dir, f\"lfp_{probe_id}.zarr\")\n",
    "\n",
    "#     with h5py.File(input_filepath, \"r\") as f:\n",
    "#         lfp_dataset = f[f\"acquisition/probe_{probe_id}_lfp/probe_{probe_id}_lfp_data/data\"]\n",
    "#         time_dataset = f[f\"acquisition/probe_{probe_id}_lfp/probe_{probe_id}_lfp_data/timestamps\"]\n",
    "#         unit_attributes = {\n",
    "#             \"time_unit\": time_dataset.attrs.get(\"unit\", None),\n",
    "#             \"lfp_unit\": lfp_dataset.attrs.get(\"unit\", None),\n",
    "#         }\n",
    "#         reference_time = f.attrs.get(\"timestamps_reference_time\", None)\n",
    "#         session_description = f.attrs.get(\"session_description\", None)\n",
    "#         subject_info = {\n",
    "#             \"species\": f[\"general/subject\"].attrs.get(\"species\", None),\n",
    "#             \"genotype\": f[\"general/subject\"].attrs.get(\"genotype\", None),\n",
    "#             \"sex\": f[\"general/subject\"].attrs.get(\"sex\", None),\n",
    "#         }\n",
    "#         electrodes_table = f[\"general/extracellular_ephys/electrodes\"]\n",
    "#         electrode_metadata = {col_name: electrodes_table[col_name][:] for col_name in electrodes_table.keys()}\n",
    "        \n",
    "#         # Creating MultiIndex for channel coordinate\n",
    "#         channel_multiindex = pd.MultiIndex.from_frame(pd.DataFrame(electrode_metadata), names=electrode_metadata.keys())\n",
    "#         channel_multiindex_coord = xr.DataArray(channel, name=\"channel\", dims=[\"channel\"], coords={\"channel\": channel_multiindex})\n",
    "        \n",
    "#         # if force_rechunk, use chunk_size.\n",
    "#         # Else, if the hdf5 has chunk info, use that; if not, use chunk_size.\n",
    "#         chunk_shape = chunk_size if force_rechunk else (lfp_dataset.chunks or chunk_size)\n",
    "#         print(f'Chunking with {chunk_shape}')\n",
    "        \n",
    "#         # Read data in chunks and concat into a Dask array (first across channels then time)\n",
    "#         arrays = []\n",
    "#         for start_time in range(0, lfp_dataset.shape[0], chunk_shape[0]):\n",
    "#             time_slice = slice(start_time, start_time + chunk_shape[0])\n",
    "#             arrays_time = [da.from_array(lfp_dataset[time_slice, slice(start_channel, start_channel + chunk_shape[1])], chunks=chunk_shape)\n",
    "#                            for start_channel in range(0, lfp_dataset.shape[1], chunk_shape[1])]\n",
    "#             arrays.append(da.concatenate(arrays_time, axis=1))\n",
    "#         lfp_dask_array = da.concatenate(arrays, axis=0)\n",
    "#         time = time_dataset[:] # load in all the time, shouldn't be too big\n",
    "#         # channel = np.arange(lfp_dataset.shape[1], dtype=np.uint32)\n",
    "\n",
    "#     ds = xr.Dataset(\n",
    "#         {\n",
    "#             \"lfp\": ([\"time\", \"channel\"], lfp_dask_array),\n",
    "#         },\n",
    "#         coords={\n",
    "#             \"time\": time,\n",
    "#             \"channel\": channel_multiindex_coord,\n",
    "#         },\n",
    "#         attrs={\n",
    "#             \"unit_attributes\": unit_attributes,\n",
    "#             \"reference_time\": reference_time,\n",
    "#             \"session_description\": session_description,\n",
    "#             \"subject_info\": subject_info,\n",
    "#             \"probe_info\": probe_info,\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     if os.path.isdir(output_filepath):\n",
    "#         shutil.rmtree(output_filepath)\n",
    "\n",
    "#     compressor = Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE) if compress_data else None\n",
    "#     ds.to_zarr(output_filepath, encoding={'lfp': {'compressor': compressor}})\n",
    "\n",
    "#     print(f\"Zarr file for probe {probe_id} has been created successfully at {output_filepath}.\")\n",
    "\n",
    "#     return ds\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
