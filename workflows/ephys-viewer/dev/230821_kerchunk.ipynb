{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce23e8b0-f86d-4ece-9b9f-ab3b304fff9e",
   "metadata": {},
   "source": [
    "## Step 1: First trying to load an xarray via kerchunked zarr refs of a fake nwb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef311937-8230-451f-90da-6e7c525953b3",
   "metadata": {},
   "source": [
    "### create fake nwb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ff5780e-15ad-4482-b2ed-4a2db7d25e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import kerchunk\n",
    "import kerchunk.hdf\n",
    "import fsspec\n",
    "import ujson\n",
    "import xarray as xr\n",
    "import warnings\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40f87003-d046-471e-b709-540813af11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_id = \"12345\"\n",
    "filename = f\"probe_{probe_id}_lfp.nwb\"\n",
    "data_group = 'data'\n",
    "ref_file = \"lfp_one_probe_ref_rand.json\"\n",
    "ref_filepath = os.getcwd() + '/' + ref_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68395764-eb8d-44f9-8881-280f1c3aee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(filename, 'w') as f:\n",
    "    probe_group = f.create_group(data_group)\n",
    "    \n",
    "    data_shape = (100, 10)\n",
    "    data = np.random.random(data_shape)\n",
    "    probe_data = probe_group.create_dataset(\"lfp\", data_shape, dtype='f', data=data, chunks=(10, 2))\n",
    "    probe_data.attrs['unit'] = 'volts'\n",
    "\n",
    "print(f\"File {filename} created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77830d51-3a3c-40c5-a6c5-f7f543be850b",
   "metadata": {},
   "source": [
    "### create kerchunk/zarr/json ref file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1434f0-4ca8-499a-a1ff-d2c0ec51e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fsspec.open(filename) as f:\n",
    "    h5chunks = kerchunk.hdf.SingleHdf5ToZarr(f, filename)\n",
    "    refs = h5chunks.translate()\n",
    "\n",
    "with open(ref_file, \"wb\") as f:\n",
    "    f.write(ujson.dumps(refs).encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fd657f-f3e1-4941-b806-6b54cecbfc5b",
   "metadata": {},
   "source": [
    "### access the data into xarray with fsspecc mapping of ref file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ca42f36-1c5d-456f-9d81-4e0f0408ebf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5514354 , 0.8056615 , 0.8522118 , 0.86789536, 0.1691171 ],\n",
       "       [0.50029904, 0.5817629 , 0.643184  , 0.24592522, 0.21853325],\n",
       "       [0.706747  , 0.95863044, 0.65548944, 0.8093027 , 0.5735166 ],\n",
       "       [0.7286443 , 0.07899158, 0.13377485, 0.60330456, 0.7686506 ],\n",
       "       [0.7145909 , 0.83703053, 0.9424319 , 0.03695546, 0.562787  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = fsspec.filesystem(\"reference\", fo=ref_filepath)\n",
    "m = fs.get_mapper()\n",
    "ds = xr.open_dataset(\n",
    "    m,\n",
    "    group=data_group,\n",
    "    engine=\"zarr\",\n",
    "    backend_kwargs={\"consolidated\": False, \"mask_and_scale\": False}\n",
    ")\n",
    "\n",
    "ds.lfp.values[:5,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589cde02-bed1-4125-930f-e1b7797a6faa",
   "metadata": {},
   "source": [
    "ok that works.. next trying on the real data but using similar approach as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b61f889-edfc-4086-86c8-cead32bd7a88",
   "metadata": {},
   "source": [
    "## Step 2: Now load the real .nwb file as is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc2c896-7d17-4d70-971a-718f0742ea85",
   "metadata": {},
   "source": [
    "- note.. xarray needs an `_ARRAY_DIMENSIONS` attribute for each dataset in order to read it from zarr\n",
    "- I'm trying to just add this `_ARRAY_DIMENSIONS` attribute right into the lfp dataset reference.\n",
    "- This is slightly different than Ian's approach of creating a zarr construct with `_ARRAY_DIMENSIONS` attribute and then fitting the lfp refs into that.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc56c7bf-3df8-41a5-baa1-ecc4cc54dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_array_dimensions(refs, data_path, dimensions):\n",
    "    ref_path = data_path + \"/.zarray\"\n",
    "    if ref_path in refs['refs']:\n",
    "        ref_str = refs['refs'][ref_path]\n",
    "        ref_dict = json.loads(ref_str)  # Deserialize the JSON string\n",
    "        print(ref_path)\n",
    "        ref_dict['_ARRAY_DIMENSIONS'] = dimensions\n",
    "        ref_str = json.dumps(ref_dict)  # Serialize back to a JSON string\n",
    "        refs['refs'][ref_path] = ref_str  # Replace the reference in refs\n",
    "        print(ref_dict)  # Original reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b2101c-0880-45e4-8226-3d52d5ea40de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acquisition/probe_810755797_lfp/probe_810755797_lfp_data/data/.zarray\n",
      "{'chunks': [41859, 1], 'compressor': {'id': 'zlib', 'level': 9}, 'dtype': '<f4', 'fill_value': 0.0, 'filters': None, 'order': 'C', 'shape': [10715666, 93], 'zarr_format': 2, '_ARRAY_DIMENSIONS': ['time', 'channel']}\n",
      "acquisition/probe_810755797_lfp/probe_810755797_lfp_data/electrodes/.zarray\n",
      "{'chunks': [93], 'compressor': None, 'dtype': '<i8', 'fill_value': 0, 'filters': None, 'order': 'C', 'shape': [93], 'zarr_format': 2, '_ARRAY_DIMENSIONS': ['channel']}\n",
      "acquisition/probe_810755797_lfp/probe_810755797_lfp_data/timestamps/.zarray\n",
      "{'chunks': [10465], 'compressor': {'id': 'zlib', 'level': 9}, 'dtype': '<f8', 'fill_value': 0.0, 'filters': None, 'order': 'C', 'shape': [10715666], 'zarr_format': 2, '_ARRAY_DIMENSIONS': ['time']}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "conflicting sizes for dimension 'phony_dim_0': length 93 on 'electrodes' and length 10715666 on {'phony_dim_0': 'data', 'phony_dim_1': 'data'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(ujson\u001b[38;5;241m.\u001b[39mdumps(refs)\u001b[38;5;241m.\u001b[39mencode())\n\u001b[1;32m     26\u001b[0m fs \u001b[38;5;241m=\u001b[39m fsspec\u001b[38;5;241m.\u001b[39mfilesystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m\"\u001b[39m, fo\u001b[38;5;241m=\u001b[39mnwb_ref_filepath)\n\u001b[0;32m---> 27\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzarr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlfp_group_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# i think setting the group is one aspect that Ian's approach was missing.. \u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconsolidated\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmask_and_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m data \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mlfp[:\u001b[38;5;241m100\u001b[39m, :\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==> data\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m.\u001b[39mcompute())\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/xarray/backends/api.py:570\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    559\u001b[0m     decode_cf,\n\u001b[1;32m    560\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    566\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    567\u001b[0m )\n\u001b[1;32m    569\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 570\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    577\u001b[0m     backend_ds,\n\u001b[1;32m    578\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    589\u001b[0m )\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/xarray/backends/zarr.py:949\u001b[0m, in \u001b[0;36mZarrBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, synchronizer, consolidated, chunk_store, storage_options, stacklevel, zarr_version)\u001b[0m\n\u001b[1;32m    947\u001b[0m store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n\u001b[0;32m--> 949\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mstore_entrypoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_characters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cftime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/xarray/backends/store.py:58\u001b[0m, in \u001b[0;36mStoreBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta)\u001b[0m\n\u001b[1;32m     44\u001b[0m encoding \u001b[38;5;241m=\u001b[39m filename_or_obj\u001b[38;5;241m.\u001b[39mget_encoding()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mvars\u001b[39m, attrs, coord_names \u001b[38;5;241m=\u001b[39m conventions\u001b[38;5;241m.\u001b[39mdecode_cf_variables(\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mvars\u001b[39m,\n\u001b[1;32m     48\u001b[0m     attrs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     decode_timedelta\u001b[38;5;241m=\u001b[39mdecode_timedelta,\n\u001b[1;32m     56\u001b[0m )\n\u001b[0;32m---> 58\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mset_coords(coord_names\u001b[38;5;241m.\u001b[39mintersection(\u001b[38;5;28mvars\u001b[39m))\n\u001b[1;32m     60\u001b[0m ds\u001b[38;5;241m.\u001b[39mset_close(filename_or_obj\u001b[38;5;241m.\u001b[39mclose)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/xarray/core/dataset.py:652\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, data_vars, coords, attrs)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(coords, Dataset):\n\u001b[1;32m    650\u001b[0m     coords \u001b[38;5;241m=\u001b[39m coords\u001b[38;5;241m.\u001b[39m_variables\n\u001b[0;32m--> 652\u001b[0m variables, coord_names, dims, indexes, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_data_and_coords\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbroadcast_equals\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    654\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(attrs) \u001b[38;5;28;01mif\u001b[39;00m attrs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/xarray/core/merge.py:569\u001b[0m, in \u001b[0;36mmerge_data_and_coords\u001b[0;34m(data_vars, coords, compat, join)\u001b[0m\n\u001b[1;32m    567\u001b[0m objects \u001b[38;5;241m=\u001b[39m [data_vars, coords]\n\u001b[1;32m    568\u001b[0m explicit_coords \u001b[38;5;241m=\u001b[39m coords\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge_core\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplicit_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplicit_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIndexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/xarray/core/merge.py:761\u001b[0m, in \u001b[0;36mmerge_core\u001b[0;34m(objects, compat, join, combine_attrs, priority_arg, explicit_coords, indexes, fill_value)\u001b[0m\n\u001b[1;32m    756\u001b[0m prioritized \u001b[38;5;241m=\u001b[39m _get_priority_vars_and_indexes(aligned, priority_arg, compat\u001b[38;5;241m=\u001b[39mcompat)\n\u001b[1;32m    757\u001b[0m variables, out_indexes \u001b[38;5;241m=\u001b[39m merge_collected(\n\u001b[1;32m    758\u001b[0m     collected, prioritized, compat\u001b[38;5;241m=\u001b[39mcompat, combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs\n\u001b[1;32m    759\u001b[0m )\n\u001b[0;32m--> 761\u001b[0m dims \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    763\u001b[0m coord_names, noncoord_names \u001b[38;5;241m=\u001b[39m determine_coords(coerced)\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m explicit_coords \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/xarray/core/variable.py:3282\u001b[0m, in \u001b[0;36mcalculate_dimensions\u001b[0;34m(variables)\u001b[0m\n\u001b[1;32m   3280\u001b[0m             last_used[dim] \u001b[38;5;241m=\u001b[39m k\n\u001b[1;32m   3281\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m dims[dim] \u001b[38;5;241m!=\u001b[39m size:\n\u001b[0;32m-> 3282\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3283\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconflicting sizes for dimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3284\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m and length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdims[dim]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_used\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3285\u001b[0m             )\n\u001b[1;32m   3286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dims\n",
      "\u001b[0;31mValueError\u001b[0m: conflicting sizes for dimension 'phony_dim_0': length 93 on 'electrodes' and length 10715666 on {'phony_dim_0': 'data', 'phony_dim_1': 'data'}"
     ]
    }
   ],
   "source": [
    "data_dir='~/data/allen/'\n",
    "data_dir = os.path.expanduser(data_dir)\n",
    "probe_id = \"810755797\"\n",
    "nwb_filepath = os.path.join(data_dir, f\"probe_{probe_id}_lfp.nwb\")\n",
    "nwb_ref_filepath = os.path.join(data_dir, \"lfp_one_probe_ref.json\")\n",
    "lfp_group_path = f\"acquisition/probe_{probe_id}_lfp/probe_{probe_id}_lfp_data\"\n",
    "lfp_data_path = lfp_group_path + \"/data\"\n",
    "electrodes_data_path = lfp_group_path + \"/electrodes\"\n",
    "time_data_path = lfp_group_path + \"/timestamps\"\n",
    "\n",
    "with fsspec.open(nwb_filepath) as f:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        h5chunks = kerchunk.hdf.SingleHdf5ToZarr(f, nwb_filepath)\n",
    "        refs = h5chunks.translate()\n",
    "\n",
    "# Adding _ARRAY_DIMENSIONS\n",
    "set_array_dimensions(refs, lfp_data_path, [\"time\", \"channel\"])\n",
    "set_array_dimensions(refs, electrodes_data_path, [\"channel\"])\n",
    "set_array_dimensions(refs, time_data_path, [\"time\"])\n",
    "\n",
    "\n",
    "with open(nwb_ref_filepath, \"wb\") as f:\n",
    "    f.write(ujson.dumps(refs).encode())\n",
    "\n",
    "fs = fsspec.filesystem(\"reference\", fo=nwb_ref_filepath)\n",
    "ds = xr.open_dataset(\n",
    "    fs.get_mapper(),\n",
    "    engine=\"zarr\",\n",
    "    group=lfp_group_path, # i think setting the group is one aspect that Ian's approach was missing.. \n",
    "    backend_kwargs={\"consolidated\": False, \"mask_and_scale\": False}\n",
    ")\n",
    "\n",
    "data = ds.lfp[:100, :4]\n",
    "print(\"==> data\", data.compute())\n",
    "mean = data.mean().compute()\n",
    "print(\"==> mean\", mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a55d22c-0890-4406-a07c-778d71913019",
   "metadata": {},
   "source": [
    "`RuntimeError: error during blosc decompression: -1`.. maybe the underlying data is not blosc..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "584952f4-99e4-4c4f-ad16-365e576fc24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression method: gzip\n",
      "Compression options: 9\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(nwb_filepath, 'r') as f:\n",
    "    dataset = f[lfp_data_path]\n",
    "    \n",
    "    print(\"Compression method:\", dataset.compression)\n",
    "    print(\"Compression options:\", dataset.compression_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddad786-ee41-450a-b19c-a533fc57140d",
   "metadata": {},
   "source": [
    "I'm not sure how to deal with this information. would it go into the kerchunk references? or xarray reading?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149bb05-0fe5-4108-94c2-5b3e3683d64b",
   "metadata": {},
   "source": [
    "Now I'm getting: `ValueError: conflicting sizes for dimension 'phony_dim_0': length 93 on 'electrodes' and length 10715666 on {'phony_dim_0': 'data', 'phony_dim_1': 'data'}`... which I don't undertstand because I'm setting the dims with the `_Array_Dimensions` field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb29bf-5184-44af-a0a1-7a1d8a99556d",
   "metadata": {},
   "source": [
    "## Step 1: KERCHUNK! Create Zarr Directory with References \n",
    "- (create_zarr_lfp_one_probe_ref.py)\n",
    "- Import the required libraries and set up the configuration for creating a Zarr directory containing references to locally-stored probe NWB files.\n",
    "- get input filename, sizes\n",
    "- create a skeleton Zarr mapping of datasets with lfp, time, and channel. populate the time array and sim the channel.\n",
    "- get references from kerchunked NWB and add the lfp references to the \n",
    "- write the references to a JSON file\n",
    "- Add references to lfp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95990b9c-8895-4559-afa6-e514a9e61484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import h5py\n",
    "import kerchunk.hdf\n",
    "import kerchunk.utils\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import ujson\n",
    "import warnings\n",
    "import zarr\n",
    "\n",
    "# Configuration\n",
    "data_dir='~/data/allen/'\n",
    "data_dir = os.path.expanduser(data_dir)\n",
    "probe_id = \"810755797\"\n",
    "\n",
    "input_directory = os.path.join(data_dir)\n",
    "output_json_file = os.path.join(data_dir, \"lfp_one_probe_ref.json\")\n",
    "\n",
    "\n",
    "def get_input_filename(probe_id):\n",
    "    return os.path.join(input_directory, f\"probe_{probe_id}_lfp.nwb\")\n",
    "\n",
    "\n",
    "def get_sizes():\n",
    "    chunk_size = dict(probe=1)\n",
    "\n",
    "    f = h5py.File(get_input_filename(probe_id), \"r\")\n",
    "    lfp = f[f\"acquisition/probe_{probe_id}_lfp/probe_{probe_id}_lfp_data/data\"]\n",
    "    ntime, nchannel = lfp.shape\n",
    "    print(ntime, nchannel)\n",
    "\n",
    "    lfp_dtype = lfp.dtype\n",
    "\n",
    "    time = f[f\"acquisition/probe_{probe_id}_lfp/probe_{probe_id}_lfp_data/timestamps\"]\n",
    "    time_dtype = time.dtype\n",
    "\n",
    "    chunk_size = dict(time=lfp.chunks[0], channel=lfp.chunks[1])\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    print(\"ntime\", ntime, \"nchannel\", nchannel)\n",
    "    print(\"Chunk sizes\", chunk_size)\n",
    "    print(\"Number of time chunks\", math.ceil(ntime / chunk_size[\"time\"]))\n",
    "\n",
    "    return ntime, nchannel, lfp_dtype, time_dtype, chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943eaf36-494f-42c2-96a4-11db29b34ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10715666 93\n",
      "ntime 10715666 nchannel 93\n",
      "Chunk sizes {'time': 41859, 'channel': 1}\n",
      "Number of time chunks 256\n"
     ]
    }
   ],
   "source": [
    "ntime, nchannel, lfp_dtype, time_dtype, chunk_size = get_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5653c5ab-7e39-49f0-bc6c-e006e1cfe22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing time for probe 810755797\n",
      "==> CHECK k acquisition/probe_810755797_lfp/probe_810755797_lfp_data/data/20.2\n",
      "==>       v ['/Users/droumis/data/allen/probe_810755797_lfp.nwb', 163354739, 84801]\n",
      "==>       name lfp/20.2\n",
      "Writing /Users/droumis/data/allen/lfp_one_probe_ref.json\n"
     ]
    }
   ],
   "source": [
    "def create_zarr_file(ntime, nchannel, lfp_dtype, time_dtype, chunk_size):\n",
    "    shape = (ntime, nchannel)\n",
    "    refs = {}\n",
    "\n",
    "    root = zarr.open_group(refs, mode=\"w\")\n",
    "    \n",
    "    # LFP data\n",
    "    #### Probably needs to know compression of underlying data?????????\n",
    "    ## DR: compression of underlying data is gzip level 9.. but I don't know how to use this info\n",
    "    lfp_data = root.create_dataset(\n",
    "        name=\"lfp\",\n",
    "        shape=shape,\n",
    "        synchronizer=zarr.ThreadSynchronizer(),\n",
    "        chunks=(chunk_size[\"time\"], chunk_size[\"channel\"]),\n",
    "        dtype=lfp_dtype,\n",
    "    )\n",
    "    lfp_data.attrs[\"_ARRAY_DIMENSIONS\"] = [\"time\", \"channel\"]\n",
    "    \n",
    "    # Time coordinates, filled in later.\n",
    "    time_data = root.create_dataset(\n",
    "        name=\"time\",\n",
    "        chunks=chunk_size[\"time\"],  # Do I want this chunked or not?\n",
    "        shape=ntime,\n",
    "        dtype=time_dtype,\n",
    "    )\n",
    "    time_data.attrs[\"_ARRAY_DIMENSIONS\"] = [\"time\"]\n",
    "\n",
    "    # Channel coordinates are integers starting at zero.\n",
    "    channel = np.arange(nchannel, dtype=np.uint32)\n",
    "    coord = root.create_dataset(\n",
    "        name=\"channel\",\n",
    "        shape=nchannel,\n",
    "        dtype=channel.dtype,\n",
    "    )\n",
    "    coord[:] = channel\n",
    "    coord.attrs[\"_ARRAY_DIMENSIONS\"] = [\"channel\"]\n",
    "\n",
    "    return time_data, refs\n",
    "\n",
    "\n",
    "def get_kerchunk_refs(probe_id, refs):\n",
    "    filename = get_input_filename(probe_id)\n",
    "    so = dict(anon=True, default_fill_cache=False, default_cache_type='first')\n",
    "\n",
    "    # Getting all the chunk references from the NWB file here, then filtering them.\n",
    "    # Can I just read the ones I am interested in?\n",
    "    with fsspec.open(filename, **so) as f:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            h5chunks = kerchunk.hdf.SingleHdf5ToZarr(f, filename)\n",
    "            probe_refs = h5chunks.translate()[\"refs\"]\n",
    "\n",
    "    # Filter refs to only contain what we want\n",
    "    # Don't know if need .zarray and .zattrs or not.\n",
    "    match = f\"acquisition/probe_{probe_id}_lfp/probe_{probe_id}_lfp_data/data/\"\n",
    "    for ref in list(probe_refs):\n",
    "        if not ref.startswith(match):\n",
    "            probe_refs.pop(ref)\n",
    "\n",
    "    len_match = len(match)\n",
    "    for k, v in probe_refs.items():\n",
    "        suffix = k[len_match:]\n",
    "        # Don't think .zarray and .zattrs are needed as the create_dataset\n",
    "        # specifies the dtype, shape, etc\n",
    "        if suffix[0] == \".\":\n",
    "            continue\n",
    "        # before, this was refs[f\"lfo... typo?\n",
    "        refs[f\"lfp/{suffix}\"] = v\n",
    "\n",
    "        if suffix == \"20.2\":\n",
    "            print(\"==> CHECK k\", k)\n",
    "            print(\"==>       v\", v)\n",
    "            print(\"==>       name\", f\"lfp/{suffix}\")\n",
    "\n",
    "\n",
    "def load_and_store(time2d_data, refs):\n",
    "    input_filename = get_input_filename(probe_id)\n",
    "    f = h5py.File(input_filename, \"r\")\n",
    "\n",
    "    print(\"Writing time for probe\", probe_id)\n",
    "    time = f[f\"acquisition/probe_{probe_id}_lfp/probe_{probe_id}_lfp_data/timestamps\"]\n",
    "    time2d_data[:] = time[:]\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    # LFP data kept in original files, referenced chunkwise from kerchunk-created JSON file\n",
    "    get_kerchunk_refs(probe_id, refs)\n",
    "\n",
    "    refs = kerchunk.utils.consolidate(refs)\n",
    "    return refs\n",
    "\n",
    "time2d_data, refs = create_zarr_file(ntime, nchannel, lfp_dtype, time_dtype, chunk_size)\n",
    "\n",
    "refs = load_and_store(time2d_data, refs)\n",
    "\n",
    "print(f\"Writing {output_json_file}\")\n",
    "with open(output_json_file, \"w\") as f:\n",
    "    ujson.dump(refs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f40ad6-26d7-48fb-b0ed-11f81fdae4bd",
   "metadata": {},
   "source": [
    "## Step 2: KERCHUNK! Data Access and Calculations\n",
    "- (test_zarr_lfp_one_probe_ref.py)\n",
    "- Open the JSON/Zarr file created in the previous step and print the dataset's structure.\n",
    "- Try some data access and calculations on the LFP data read from the JSON/Zarr file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d7ce2-d9df-4ad2-8f04-3bb5f994efa7",
   "metadata": {},
   "source": [
    "I'm stuck at: \"RuntimeError: error during blosc decompression: -1\".. Maybe we do need to use the compression of underlying data in some way (gzip level 9).. I need Ian's help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "842acc3e-fb0c-40ac-9750-c187ac47f9e3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (channel: 93, time: 10715666)\n",
      "Coordinates:\n",
      "  * channel  (channel) uint32 0 1 2 3 4 5 6 7 8 9 ... 84 85 86 87 88 89 90 91 92\n",
      "  * time     (time) float64 28.82 28.82 28.83 ... 9.616e+03 9.616e+03 9.616e+03\n",
      "Data variables:\n",
      "    lfp      (time, channel) float32 ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "error during blosc decompression: -1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Try some data access and calculations\u001b[39;00m\n\u001b[1;32m     20\u001b[0m data \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mlfp[:\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==> data\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     22\u001b[0m mean \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==> mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, mean)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/xarray/core/dataarray.py:1102\u001b[0m, in \u001b[0;36mDataArray.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;124;03mremote source into memory and return a new array. The original is\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;124;03mleft unaltered.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;124;03mdask.compute\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/xarray/core/dataarray.py:1076\u001b[0m, in \u001b[0;36mDataArray.load\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m: T_DataArray, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T_DataArray:\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;124;03m    remote source into memory and return this array.\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1076\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_temp_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1077\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_temp_dataset(ds)\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable \u001b[38;5;241m=\u001b[39m new\u001b[38;5;241m.\u001b[39m_variable\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/xarray/core/dataset.py:800\u001b[0m, in \u001b[0;36mDataset.load\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m lazy_data:\n\u001b[0;32m--> 800\u001b[0m         \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/xarray/core/variable.py:546\u001b[0m, in \u001b[0;36mVariable.load\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m as_compatible_data(loaded_data)\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, indexing\u001b[38;5;241m.\u001b[39mExplicitlyIndexed):\n\u001b[0;32m--> 546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_duck_array(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data):\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/xarray/core/indexing.py:696\u001b[0m, in \u001b[0;36mMemoryCachedArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 696\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mget_duck_array()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/xarray/core/indexing.py:690\u001b[0m, in \u001b[0;36mMemoryCachedArray._ensure_cached\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_cached\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m as_indexable(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/xarray/core/indexing.py:664\u001b[0m, in \u001b[0;36mCopyOnWriteArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/xarray/core/indexing.py:551\u001b[0m, in \u001b[0;36mLazilyIndexedArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 551\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;66;03m# self.array[self.key] is now a numpy array when\u001b[39;00m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;66;03m# self.array is a BackendArray subclass\u001b[39;00m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;66;03m# and self.key is BasicIndexer((slice(None, None, None),))\u001b[39;00m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;66;03m# so we need the explicit check for ExplicitlyIndexed\u001b[39;00m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, ExplicitlyIndexed):\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/xarray/backends/zarr.py:92\u001b[0m, in \u001b[0;36mZarrArrayWrapper.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     90\u001b[0m array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_array()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, indexing\u001b[38;5;241m.\u001b[39mBasicIndexer):\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuple\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, indexing\u001b[38;5;241m.\u001b[39mVectorizedIndexer):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\u001b[38;5;241m.\u001b[39mvindex[\n\u001b[1;32m     95\u001b[0m         indexing\u001b[38;5;241m.\u001b[39m_arrayize_vectorized_indexer(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m.\u001b[39mtuple\n\u001b[1;32m     96\u001b[0m     ]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/zarr/core.py:824\u001b[0m, in \u001b[0;36mArray.__getitem__\u001b[0;34m(self, selection)\u001b[0m\n\u001b[1;32m    822\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvindex[selection]\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_pure_orthogonal_indexing(pure_selection, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim):\n\u001b[0;32m--> 824\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_orthogonal_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpure_selection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    826\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_basic_selection(pure_selection, fields\u001b[38;5;241m=\u001b[39mfields)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/zarr/core.py:1106\u001b[0m, in \u001b[0;36mArray.get_orthogonal_selection\u001b[0;34m(self, selection, out, fields)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# setup indexer\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m indexer \u001b[38;5;241m=\u001b[39m OrthogonalIndexer(selection, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/zarr/core.py:1284\u001b[0m, in \u001b[0;36mArray._get_selection\u001b[0;34m(self, indexer, out, fields)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m math\u001b[38;5;241m.\u001b[39mprod(out_shape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;66;03m# allow storage to get multiple items at once\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m     lchunk_coords, lchunk_selection, lout_selection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mindexer)\n\u001b[0;32m-> 1284\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chunk_getitems\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlchunk_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlchunk_selection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlout_selection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/zarr/core.py:2032\u001b[0m, in \u001b[0;36mArray._chunk_getitems\u001b[0;34m(self, lchunk_coords, lchunk_selection, out, lout_selection, drop_axes, fields)\u001b[0m\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ckey, chunk_select, out_select \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ckeys, lchunk_selection, lout_selection):\n\u001b[1;32m   2031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ckey \u001b[38;5;129;01min\u001b[39;00m cdatas:\n\u001b[0;32m-> 2032\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_chunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m            \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2034\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcdatas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mckey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk_select\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2037\u001b[0m \u001b[43m            \u001b[49m\u001b[43mout_is_ndarray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2039\u001b[0m \u001b[43m            \u001b[49m\u001b[43mout_select\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2040\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpartial_read_decode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial_read_decode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2043\u001b[0m         \u001b[38;5;66;03m# check exception type\u001b[39;00m\n\u001b[1;32m   2044\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/zarr/core.py:1946\u001b[0m, in \u001b[0;36mArray._process_chunk\u001b[0;34m(self, out, cdata, chunk_selection, drop_axes, out_is_ndarray, fields, out_selection, partial_read_decode)\u001b[0m\n\u001b[1;32m   1944\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ArrayIndexError:\n\u001b[1;32m   1945\u001b[0m     cdata \u001b[38;5;241m=\u001b[39m cdata\u001b[38;5;241m.\u001b[39mread_full()\n\u001b[0;32m-> 1946\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# select data from chunk\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fields:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-eeg-viewer/lib/python3.9/site-packages/zarr/core.py:2202\u001b[0m, in \u001b[0;36mArray._decode_chunk\u001b[0;34m(self, cdata, start, nitems, expected_shape)\u001b[0m\n\u001b[1;32m   2200\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compressor\u001b[38;5;241m.\u001b[39mdecode_partial(cdata, start, nitems)\n\u001b[1;32m   2201\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2202\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2204\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m cdata\n",
      "File \u001b[0;32mnumcodecs/blosc.pyx:564\u001b[0m, in \u001b[0;36mnumcodecs.blosc.Blosc.decode\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mnumcodecs/blosc.pyx:394\u001b[0m, in \u001b[0;36mnumcodecs.blosc.decompress\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: error during blosc decompression: -1"
     ]
    }
   ],
   "source": [
    "import fsspec\n",
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "input_json = output_json_file\n",
    "# input_json = os.path.join(directory, \"lfp_one_probe_ref.json\")\n",
    "\n",
    "# Disable mask_and_scale otherwise dtypes are converted to floats.\n",
    "fs = fsspec.filesystem(\"reference\", fo=input_json)\n",
    "ds = xr.open_dataset(\n",
    "    fs.get_mapper(\"\"),\n",
    "    engine=\"zarr\",\n",
    "    group=\"\",\n",
    "    backend_kwargs={\"consolidated\": False, \"mask_and_scale\": False}\n",
    ")\n",
    "print(ds)\n",
    "\n",
    "\n",
    "# Try some data access and calculations\n",
    "data = ds.lfp[:100, 0]\n",
    "print(\"==> data\", data.compute())\n",
    "mean = data.mean().compute()\n",
    "print(\"==> mean\", mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2918fa-6668-4669-bed8-6bce229f2249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b598266-11c6-4ac2-b6ae-a0fe974c5bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e96644b-b2d5-4714-a4d5-9eebe623d18a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7df637eb-ab37-4e3a-bd06-1d6dfa8fc04e",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8759a28f-08de-4dd8-a668-52fb100acd93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93dca8-d6d7-41af-acf7-772d26e715ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# import numpy as np\n",
    "\n",
    "# probe_id = \"12345\"\n",
    "# filename = f\"probe_{probe_id}_lfp.nwb\"\n",
    "# # data_group = f\"acquisition/probe_{probe_id}_lfp/probe_{probe_id}_lfp_data\"\n",
    "# data_group = 'lfp'\n",
    "\n",
    "# # Create a new HDF5 file\n",
    "# with h5py.File(filename, 'w') as f:\n",
    "#     # Create a group structure\n",
    "#     probe_group = f.create_group(data_group)\n",
    "    \n",
    "#     # Create a dataset with random data\n",
    "#     data_shape = (100, 10)\n",
    "#     data = np.random.random(data_shape)\n",
    "#     probe_data = probe_group.create_dataset(\"data\", data_shape, dtype='f', data=data, chunks=(10, 2))\n",
    "    \n",
    "#     # Add attributes (Optional)\n",
    "#     probe_data.attrs['unit'] = 'volts'\n",
    "\n",
    "# print(f\"File {filename} created.\")\n",
    "\n",
    "\n",
    "# import kerchunk.hdf\n",
    "# import fsspec\n",
    "# import ujson\n",
    "\n",
    "# # so = dict(anon=True, default_fill_cache=False, default_cache_type='first')\n",
    "\n",
    "# # Open the HDF5 file and translate the chunk information\n",
    "# # with fsspec.open(filename, **so) as f:\n",
    "# with fsspec.open(filename) as f:\n",
    "#     h5chunks = kerchunk.hdf.SingleHdf5ToZarr(f, filename)\n",
    "#     refs = h5chunks.translate()#[\"refs\"] #translates content of the HDF5 file into the Zarr format\n",
    "\n",
    "# # Consolidate the references\n",
    "# # refs = kerchunk.utils.consolidate(refs)\n",
    "\n",
    "# # Print the references\n",
    "# # print(refs)\n",
    "\n",
    "# Save to a JSON file\n",
    "# output_json_file = \"lfp_one_probe_ref_rand.json\"\n",
    "# with open(output_json_file, \"wb\") as f:\n",
    "#     # ujson.dump(refs, f)\n",
    "#     f.write(ujson.dumps(refs).encode())\n",
    "\n",
    "# with open(output_json_file, \"r\") as f:\n",
    "#     content = f.read()\n",
    "#     print(content)\n",
    "\n",
    "# with fsspec.open(url, **so) as inf:\n",
    "#     h5chunks = kerchunk.hdf.SingleHdf5ToZarr(inf, url, inline_threshold=100)\n",
    "#     h5chunks.translate()\n",
    "#     with open(\"single_file_kerchunk.json\", \"wb\") as f:\n",
    "#         f.write(ujson.dumps(h5chunks.translate()).encode()\n",
    "\n",
    "# # import os\n",
    "\n",
    "# # data_dir='~/data/allen/'\n",
    "# json_filepath = '/Users/droumis/src/neuro/workflows/ephys-viewer/dev/' + output_json_file\n",
    "# # data_dir = os.path.expanduser(data_dir)\n",
    "# # output_json_file = os.path.join(data_dir, output_json_file)\n",
    "# json_filepath\n",
    "\n",
    "# import fsspec\n",
    "# import xarray as xr\n",
    "\n",
    "# # data_group = f\"acquisition/probe_{probe_id}_lfp/probe_{probe_id}_lfp_data\"\n",
    "# # ref_data_group = 'refs/refs/' + data_group\n",
    "# # Disable mask_and_scale otherwise dtypes are converted to floats.\n",
    "# fs = fsspec.filesystem(\"reference\", fo=json_filepath)\n",
    "# m = fs.get_mapper()\n",
    "# ds = xr.open_dataset(\n",
    "#     m,\n",
    "#     group='lfp',\n",
    "#     engine=\"zarr\",\n",
    "#     backend_kwargs={\"consolidated\": False, \"mask_and_scale\": False}\n",
    "# )\n",
    "\n",
    "# print(ds)\n",
    "\n",
    "# # Try some data access and calculations\n",
    "# data = ds.data[:100, 0]  # Update this line according to the actual structure of your data\n",
    "# print(\"==> data\", data.compute())\n",
    "# mean = data.mean().compute()\n",
    "# print(\"==> mean\", mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
